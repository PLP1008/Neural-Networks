{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE 1"
      ],
      "metadata": {
        "id": "UyrzwQl3Hgbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I USED NEURAL NETS HERE.\n",
        "AND YES, THIS IS BETTER THAN BIGRAM MODEL.\n"
      ],
      "metadata": {
        "id": "rkVA_7lAIh60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "words = open(\"names.txt\",\"r\").read().splitlines()\n",
        "chars = sorted(list(set('.'.join(words))))"
      ],
      "metadata": {
        "id": "yEigZQQA7gnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi={s:i for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos={i:s for s,i in stoi.items()}\n"
      ],
      "metadata": {
        "id": "izYTvEDq7zVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#created bitoi, itobi for convenience\n",
        "xs = []\n",
        "ys = []\n",
        "b =[]\n",
        "for ch in chars:\n",
        "  for char in chars:\n",
        "    b.append((ch,char))\n",
        "\n",
        "bi = sorted(list(set(b)))\n",
        "bitoi = {(ch1,ch2):i for i, (ch1,ch2) in enumerate(bi)}\n",
        "itobi = {i:s for s,i in bitoi.items()}\n",
        "\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        xs.append(bitoi[(ch1, ch2)])\n",
        "        ys.append(stoi[ch3])\n"
      ],
      "metadata": {
        "id": "ttR7Sdl6bu2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "print(xs)\n",
        "print(ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CLMPm-qfoAd",
        "outputId": "ecbafab9-0dff-47d7-8ffb-9d9ec75833d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  5, 148, 364,  ..., 727, 701, 726])\n",
            "tensor([13, 13,  1,  ..., 26, 24,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes =729).float()\n",
        "print(xenc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTgaaQDsBWTB",
        "outputId": "d353016d-d0cd-43db-b1f0-e40aa322d2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 1., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = xs.nelement()\n",
        "print(num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOivHkKMv5og",
        "outputId": "73b307b7-b09a-4253-e01e-42ad0ae20e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g= torch.Generator().manual_seed(2147483652)\n",
        "W = torch.randn((729,27),generator =g, requires_grad = True)"
      ],
      "metadata": {
        "id": "2EdjrENvvsJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss calculation\n",
        "for k in range(10):\n",
        "\n",
        "  xenc = F.one_hot(xs, num_classes=729).float()\n",
        "  logits = xenc @ W\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  #print(probs.shape)\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  W.data += -100 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_sLDQcIvngL",
        "outputId": "ba8420a1-d8f5-4bd0-faa6-45a41e48de5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5238139629364014\n",
            "2.514756679534912\n",
            "2.5060904026031494\n",
            "2.497786045074463\n",
            "2.489819288253784\n",
            "2.4821698665618896\n",
            "2.474818706512451\n",
            "2.4677488803863525\n",
            "2.4609439373016357\n",
            "2.4543895721435547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OUTPUT\n",
        "for i in range(30):\n",
        " out = []\n",
        " ix = 0\n",
        "\n",
        " while True:\n",
        "  xenc = F.one_hot(torch.tensor([ix]), num_classes =729).float()\n",
        "  logits = xenc @ W\n",
        "  counts = logits.exp()\n",
        "  p = counts / counts.sum(1,keepdims = True)\n",
        "  #print(p.shape)\n",
        "  var = list(itobi[ix])\n",
        "  ixo = stoi[var[1]]\n",
        "  ix = torch.multinomial(p, num_samples=1, replacement=True, generator = g).item()\n",
        "  out.append(itos[ix])\n",
        "  ixn = ix\n",
        "  if ix==0:\n",
        "    break\n",
        "  ix = bitoi[(itos[ixo],itos[ixn])]\n",
        " print(''.join(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR9SzhRLrBvj",
        "outputId": "5cff34ca-f1b4-48b5-dd64-a66cd9f6d101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eeessrm.\n",
            "cacalin.\n",
            "ma.\n",
            "grvplxwpnuaczclqsanalistt.\n",
            "obbjbcnwmtvgel.\n",
            "cyson.\n",
            "glhnawubvznian.\n",
            "pcltintzllkbkzlgkmya.\n",
            "hadjslzbvlnysutlaznfbdjkpm.\n",
            "gry.\n",
            "na.\n",
            "miwwsxev.\n",
            "per.\n",
            "cooxamipohvvyleigbypswhckbpodnqnzisswvyzlkhymcuo.\n",
            "puzmxprxlscgh.\n",
            "brsnirybkbxaudrek.\n",
            "maj.\n",
            "lijmon.\n",
            "bribaxjnbllnyla.\n",
            "bgknvxhhanna.\n",
            "mir.\n",
            "caklmifbtfyqegyp.\n",
            "chhhadecbx.\n",
            "pedyel.\n",
            "on.\n",
            "vi.\n",
            "hashacjkrsfomaashadelzqqvjyueilqpfuckkjisthbbn.\n",
            "p.\n",
            "izahrrossco.\n",
            "ivyyleigalfkfdbfvmhmwgjcvfrjkglhbicybsxcmqrlia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 2 (TRIGRAM)"
      ],
      "metadata": {
        "id": "wpbed6SfH6HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss function to calculate loss seperately for diff datasets"
      ],
      "metadata": {
        "id": "zSAvywaWNdkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(xs,ys):\n",
        "  num = xs.nelement()\n",
        "  xenc = F.one_hot(xs, num_classes=729).float()\n",
        "  logits = xenc @ W\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  #print(loss.item())\n",
        "\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  W.data += -50 * W.grad\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "7v4YCNQnH-3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "seperating dataset"
      ],
      "metadata": {
        "id": "r6WdmuMRNj5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "data = list(zip(xs, ys))\n",
        "torch.manual_seed(42)\n",
        "random.shuffle(data)\n",
        "\n",
        "split1 = int(0.8 * len(data))\n",
        "split2 = int(0.9 * len(data))\n",
        "\n",
        "train_data = data[:split1]\n",
        "dev_data = data[split1:split2]\n",
        "test_data = data[split2:]\n",
        "\n",
        "#training set\n",
        "xs_train, ys_train = [], []\n",
        "for x, y in train_data:\n",
        "    xs_train.append(x)\n",
        "    ys_train.append(y)\n",
        "\n",
        "xs_train = torch.tensor(xs_train)\n",
        "ys_train = torch.tensor(ys_train)\n",
        "\n",
        "#dev set\n",
        "xs_dev, ys_dev = [], []\n",
        "for x, y in dev_data:\n",
        "    xs_dev.append(x)\n",
        "    ys_dev.append(y)\n",
        "\n",
        "xs_dev = torch.tensor(xs_dev)\n",
        "ys_dev = torch.tensor(ys_dev)\n",
        "\n",
        "#test set\n",
        "xs_test, ys_test = [], []\n",
        "for x, y in test_data:\n",
        "    xs_test.append(x)\n",
        "    ys_test.append(y)\n",
        "\n",
        "xs_test = torch.tensor(xs_test)\n",
        "ys_test = torch.tensor(ys_test)\n"
      ],
      "metadata": {
        "id": "649jelDrPKDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = loss_fn(xs_train, ys_train)\n",
        "dev_loss = loss_fn(xs_dev, ys_dev)\n",
        "test_loss = loss_fn(xs_test, ys_test)\n",
        "print(train_loss)\n",
        "print(dev_loss)\n",
        "print(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II5SBxWhRAiO",
        "outputId": "6e507c4a-32f0-402d-a42b-3a5c1b54ef6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.449626922607422\n",
            "2.4415647983551025\n",
            "2.433560609817505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, we can see that all are almost equal and also loss of dev set and test set are slightly lower than that of training set for this trigram model"
      ],
      "metadata": {
        "id": "zTKybFshTGXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 2 (BIGRAM)"
      ],
      "metadata": {
        "id": "kYHfL2IY1D5x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWnSTpu81ZF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 3"
      ],
      "metadata": {
        "id": "2RDguwOdgoOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss function to calculate loss seperately for diff datasets by varyling l for regularization"
      ],
      "metadata": {
        "id": "sDeBn_1LOMeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fun(xs,ys,l):\n",
        "  num = xs.nelement()\n",
        "  xenc = F.one_hot(xs, num_classes=729).float()\n",
        "  logits = xenc @ W\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + l*(W**2).mean()\n",
        "  #print(loss.item())\n",
        "\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  #W.data += -50 * W.grad\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "D4PFuG7tgllE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to observe loss of training set"
      ],
      "metadata": {
        "id": "rksu6RNwOP86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 0.0000001\n",
        "for i in range(8):\n",
        "  train_loss = loss_fun(xs_train, ys_train, a)\n",
        "  print(train_loss)\n",
        "  a*=10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfj1HsABshdY",
        "outputId": "ae5a2243-b07c-44c6-e225-9ec8d77ae2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.39216685295105\n",
            "2.392167806625366\n",
            "2.392176389694214\n",
            "2.3922626972198486\n",
            "2.3931264877319336\n",
            "2.4017629623413086\n",
            "2.488129138946533\n",
            "3.3517889976501465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "to observe loss of dev set"
      ],
      "metadata": {
        "id": "OBV0ltzWOdpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = 0.0000001\n",
        "for i in range(8):\n",
        " #print(\"trial :\",i)\n",
        " print(\"l = \",l)\n",
        " dev_loss = loss_fun(xs_dev, ys_dev,l)\n",
        " print(dev_loss)\n",
        " l*=10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f9a2fe-33b4-40fc-bcda-61843456b1a1",
        "id": "CmDcc7lFhuzz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l =  1e-07\n",
            "2.4240174293518066\n",
            "l =  1e-06\n",
            "2.4191877841949463\n",
            "l =  9.999999999999999e-06\n",
            "2.414562225341797\n",
            "l =  9.999999999999999e-05\n",
            "2.4101808071136475\n",
            "l =  0.001\n",
            "2.406733751296997\n",
            "l =  0.01\n",
            "2.4113123416900635\n",
            "l =  0.1\n",
            "2.4950225353240967\n",
            "l =  1.0\n",
            "3.368551731109619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And as we increase the regularization strength while finding train_loss and dev_loss, train_loss increases continuously whereas dev_loss decreases at first and increases again."
      ],
      "metadata": {
        "id": "_QuqvnTstTTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above loop, when l=0.001 dev_loss attains its minimum value"
      ],
      "metadata": {
        "id": "IXyV49X3sJVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = loss_fun(xs_test, ys_test, 0.001)\n",
        "print(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uFdMqHzvBKQ",
        "outputId": "7630ed86-71a0-49bc-ce10-5ef01175efbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8470847606658936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the test_loss with best regularization strength(\"l\")"
      ],
      "metadata": {
        "id": "k9OC1UH-vp4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 4"
      ],
      "metadata": {
        "id": "lidSOEprOT9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both the following cells, I avoided using one-hot vectors.\n",
        "The code ran succesfully and the runtime was very low compared to that using onehot vectors"
      ],
      "metadata": {
        "id": "iVOJm9_jOoIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resut of using onehot vectors was just picking out selected rows from W. So, I replaced it by writing W[xs]"
      ],
      "metadata": {
        "id": "nHNO1LQevyAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loss calculation\n",
        "for k in range(10):\n",
        "  logits = W[xs]\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  #print(probs.shape)\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  W.data += -100 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf897ff-7794-4818-f8d0-ca23d0874cc1",
        "id": "ElBQuegiSByp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8564624786376953\n",
            "2.826683759689331\n",
            "2.799455165863037\n",
            "2.7744333744049072\n",
            "2.751333236694336\n",
            "2.7299115657806396\n",
            "2.709965705871582\n",
            "2.69132137298584\n",
            "2.6738343238830566\n",
            "2.657383680343628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OUTPUT\n",
        "for i in range(30):\n",
        " out = []\n",
        " ix = 0\n",
        "\n",
        " while True:\n",
        "  logits = W[ix]\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(0,keepdims = True)\n",
        "  var = list(itobi[ix])\n",
        "  ixo = stoi[var[1]]\n",
        "  ix = torch.multinomial(probs, num_samples=1, replacement=True, generator = g).item()\n",
        "  out.append(itos[ix])\n",
        "  ixn = ix\n",
        "  if ix==0:\n",
        "    break\n",
        "  ix = bitoi[(itos[ixo],itos[ixn])]\n",
        " print(''.join(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0798ec9-2e01-48fd-b278-7740d87f8ed8",
        "id": "IwWCC4HySByp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jjbdjmon.\n",
            "brebaxjnbllnyht.\n",
            "bgknvxhhanna.\n",
            "mip.\n",
            "caklmgjbofimtlxp.\n",
            "chhhael.\n",
            "bx.\n",
            "pedyel.\n",
            "on.\n",
            "vhlhjkrpclpusfomaashadeyzqqvjyueikqpfuckkjisthbbn.\n",
            "p.\n",
            "izaurdsrmnxzivyylaidalfkfdbfvmhmwgjcvfrjkglhbicybsxcmqrlia.\n",
            "peohjrymxan.\n",
            "vdklzkwtfqqkpsgpriarrlarikton.\n",
            "phell.\n",
            "romnyjftkhykgy.\n",
            "caeorcfrtxrjp.\n",
            "brdqaqpgeyscccaptkkjwzissczwsdntgomtui.\n",
            "hael.\n",
            "helanriabpzrgdcsqawbfnplzgkncnywu.\n",
            "coyojdbs.\n",
            "ygsqpkywuxyj.\n",
            "zsyruqcvopfhvplrendael.\n",
            "wqhkvttxiimdgdhhqhzrqljlin.\n",
            "msgsnmg.\n",
            "breisfnjvqdqschavwvenicattscofnctmvuumdjlmewfjtmggfufevsxhtzpnpjylmtwgbbddrxsleisjszkmz.\n",
            "kainijwbqwskjyipfce.\n",
            "uawvuuwftpjacxgsizodakprsyoady.\n",
            "madefelazmon.\n",
            "gwspvopipwdkurubc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 5"
      ],
      "metadata": {
        "id": "_Pqo1mFQSHMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used F.cross_entropy in this cell"
      ],
      "metadata": {
        "id": "SFNOt6MNuMAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It replaced many lines of code and made it simpler so maybe that's why we prefer this"
      ],
      "metadata": {
        "id": "YUHdmYnZuXs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see below loss calculated from two methods is the same"
      ],
      "metadata": {
        "id": "vlS0QPgFuh3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loss calculation using F.cross_entropy\n",
        "logits = W[xs]\n",
        "cross_entropy_loss = F.cross_entropy(logits,ys) + 0.01*(W**2).mean()\n",
        "print(cross_entropy_loss.item())\n",
        "\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7811637d-7ba0-4c41-a35f-a8776f2b7673",
        "id": "xu6xD36_Wm23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6418673992156982\n",
            "2.6418673992156982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 6"
      ],
      "metadata": {
        "id": "jqixryAzx0JG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-catuLwAqNd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "dat = open(\"collection.txt\",\"r\").read().splitlines()\n",
        "data = dat[1:]\n",
        "chars = sorted(list(set('.'.join(data))))\n",
        "chars.insert(24,'x')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.85*len(dat))\n",
        "t_data = data[:split]\n",
        "d_data = data[split:]"
      ],
      "metadata": {
        "id": "L44adiXBGRYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs1, xs2, xs_=[],[],[]\n",
        "ys1, ys2, ys_=[],[],[]\n",
        "\n",
        "for w in t_data[:3]:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        xs1.append(bitoi[(ch1, ch2)])\n",
        "        ys1.append(stoi[ch3])\n",
        "\n",
        "for w in t_data[:3]:\n",
        "  chs_ = ['.'] + list(reversed(w)) + ['.']\n",
        "  for c1,c2,c3 in zip(chs_, chs_[1:], chs_[2:]):\n",
        "    xs2.append(bitoi[(c1,c2)])\n",
        "    ys2.append(stoi[c3])\n",
        "\n",
        "for w in d_data:\n",
        "  chsd = ['.'] +list(w) + ['.']\n",
        "  for c1,c2,c3 in zip(chsd,chsd[1:],chsd[2:]):\n",
        "    xs_.append(bitoi[(c1,c2)])\n",
        "    ys_.append(stoi[c3])"
      ],
      "metadata": {
        "id": "gRJtkuFPhnO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs1 = torch.tensor(xs1)\n",
        "ys1 = torch.tensor(ys1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys2 = torch.tensor(ys2)\n",
        "xs_ = torch.tensor(xs_)\n",
        "ys_ = torch.tensor(ys_)"
      ],
      "metadata": {
        "id": "vHaTRdmnICQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for k in range(10):\n",
        "def cal_b_loss(xs1,ys1):\n",
        "  num1 = xs1.nelement()\n",
        "  logits1 = W[xs1]\n",
        "  counts1 = logits1.exp()\n",
        "  probs1 = counts1/counts1.sum(1,keepdims = True)\n",
        "  loss1 = -probs1[torch.arange(num1),ys1].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss1.item())\n",
        "\n",
        "  W.grad = None\n",
        "  loss1.backward()\n",
        "  W.data += -50*W.grad"
      ],
      "metadata": {
        "id": "4aB-x0_oI9j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_b_loss(xs1,ys1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh1l7EWBlV_S",
        "outputId": "87a898dd-d533-4646-e91f-23c35d2763c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.18754506111145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = input()\n",
        "before = []\n",
        "after=[]\n",
        "i,j=0,0\n",
        "for q in list(question):\n",
        "  if(q=='_'):\n",
        "    for x in range(2):\n",
        "      i-=1\n",
        "      if (i>=0):\n",
        "        before.insert(0,list(question)[i])\n",
        "      else:\n",
        "        before.insert(0,'.')\n",
        "\n",
        "    for x in range(2):\n",
        "      j+=1\n",
        "      if(j<len(list(question))):\n",
        "        after.insert(0,list(question)[j])\n",
        "      else:\n",
        "        after.insert(0,'.')\n",
        "  i+=1\n",
        "  j+=1\n",
        "\n",
        "ix1 = bitoi[tuple(before)]\n",
        "ix2 = bitoi[tuple(after)]\n",
        "ix = 0\n",
        "while(ix==0):\n",
        " xenc1 = F.one_hot(torch.tensor([ix1]), num_classes=729).float()\n",
        " logits1 = xenc1 @ W\n",
        " counts1 = logits1.exp()\n",
        " probs1 = counts1/counts1.sum(1,keepdims =True)\n",
        "\n",
        " xenc2 = F.one_hot(torch.tensor([ix2]), num_classes =729).float()\n",
        " logits2 = xenc2 @ W\n",
        " counts2 = logits2.exp()\n",
        " probs2 = counts2/counts2.sum(1,keepdims = True)\n",
        "\n",
        " prob = probs1*probs2\n",
        " ix = torch.argmax(prob)\n",
        "\n",
        "ans = question.replace('_',itos[ix.item()])\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "id": "ksEyuZIDMo4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}